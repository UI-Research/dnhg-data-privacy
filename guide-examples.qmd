---
title: "Do No Harm Guide"
subtitle: "Applying Equity Awareness in Data Privacy Methods Examples"
author-title: "Authors"
authors: "Claire McKay Bowen and Joshua Snoke"
affiliation: "Urban Institute and RAND Corp."
date: "today"
format:
  html:
    theme: urbn.scss
    self-contained: true
    code-fold: true
    code-line-numbers: true
    html-math-method: katex
    df-print: default
    toc: true
    toc-depth: 3
    number-sections: true
    number-depth: 3
    highlight-style: pygments
editor_options: 
  chunk_output_type: console
execute: 
  echo: false
---

This Quarto Document walks through "Do No Harm Guide: Applying Equity Awareness in Data Privacy Methods".

**Disclaimer:** These analyses are inspired by real data and public policy analytics, but are *not* representative of them.

```{r}
#| label: load-pkgs
#| code-summary: "Packages"
#| message: false
#| warning: false
#| echo: false
library(tidyverse)  # for data wrangling and visualization
library(knitr)      # for tables
library(readxl)     # for importing excel (data)
library(smoothmest) # for laplace distribution
library(tigris)     # for census shape files
library(patchwork)  # for combining ggplots
library(urbnthemes) # for ggplot2 theme
library(doParallel) # for parallel computing
library(svglite)    # for outputing SVG files
```

## Data

Our illustrative example comes from the Urban Institute research report, "[Mapping Student Needs during COVID-19](https://www.urban.org/research/publication/mapping-student-needs-during-covid-19)". The data in the report are drawn from the American Community Survey (ACS), which is collected and published the US Census Bureau (the cleaned data can be accessed for free at the [Urban Institute Data Catalog](https://datacatalog.urban.org/dataset/household-conditions-geographic-school-district).The purpose of the research is to “…highlight different types of challenges to remote learning and point to district and educator strategies that mitigate harm to students as districts navigate long-term school closures.” In addition to poverty, the researchers examined six other factors of remote learning challenges: linguistic isolation, child disability status, parents in vulnerable economic sectors, single parent, crowded conditions, and lack of computer or broadband access. See the report for how they define these factors.

### Confidential Data
The researchers used 2014 through 2018 data from the American Community Survey  to conduct their analysis. The researchers used two public use files from the American Community Survey, but we focus on the 2014 through 2018 five-year estimate from the National Historical Geographic Information System. This data set has seven variables of interest, measured as proportions and each published with margins of error estimated by the Census Bureau. The published American Community Survey data are public data that have undergone SDP protections prior to being released.
We do not have access to the original survey data, but for purposes of this case study, we are going to treat the public use files as “confidential.” We define these confidential data as the cleaned version of the data, meaning the data have been edited for inaccuracies or inconsistencies. For simplicity, we also assume the percentages are the true values of the data, and we created another version of the data into tabular values based on those percentages. We then readjust the values to ensure there are whole counts. As an example, if a school district has a 10 percent poverty rate and 48 total students, we then list 5 students in poverty for the tabular data. 

For this guide, we highlight two states, New Mexico and Pennsylvania. We pick these states because they vary greatly in population density and the proportion of the seven variables. In 2022, 2.1 million people lived in New Mexico and 13.0 million people lived in Pennsylvania, making Pennsylvania about six times as populous as New Mexico (and almost 17 times more densely populated). As you will see, this comparison will succinctly demonstrate how population density affects statistical data privacy results. 

We make the caveat that the results shown here are intended only to help understand the potential varying effects on different groups in the data when applying different SDP techniques. We do not intend for readers to draw substantial conclusions about the merits of particular methods compared with one another or the effect of adding noise through SDP methods relative to the other forms of noise in the estimates (e.g., measurement error or sampling error).


**Disclaimer:** Please note that the analyses in this chapter are inspired by real data and public policy analytics but are not representative of them.

```{r}
#| label: data
#| code-summary: "data"
#| message: false
#| warning: false

# Proportions
nm_prop <- read_excel("data/data/nhgis_district_data_var.xlsx") %>%
  filter(state == "New Mexico") %>%
  dplyr::select(
    geographic_school_district,
    children_5_17,
    poverty,
    linguistically_isolated,
    children_disability,
    vulnerable_job,
    single_parent,
    crowded_conditions, 
    no_computer_internet
  )

pa_prop <- read_excel("data/data/nhgis_district_data_var.xlsx") %>%
  filter(state == "Pennsylvania") %>%
  dplyr::select(
    geographic_school_district,
    children_5_17,
    poverty,
    linguistically_isolated,
    children_disability,
    vulnerable_job,
    single_parent,
    crowded_conditions, 
    no_computer_internet
  ) %>%
  head(-1) # Empty district

# Tabular Data
nm_counts <- nm_prop %>%
  mutate(
    poverty = round(poverty * children_5_17),
    linguistically_isolated = round(linguistically_isolated * children_5_17),
    children_disability = round(children_disability * children_5_17),
    vulnerable_job = round(vulnerable_job * children_5_17),
    single_parent = round(single_parent * children_5_17),
    crowded_conditions = round(crowded_conditions * children_5_17), 
    no_computer_internet = round(no_computer_internet * children_5_17)
  )

pa_counts <- pa_prop %>%
  mutate(
    poverty = round(poverty * children_5_17),
    linguistically_isolated = round(linguistically_isolated * children_5_17),
    children_disability = round(children_disability * children_5_17),
    vulnerable_job = round(vulnerable_job * children_5_17),
    single_parent = round(single_parent * children_5_17),
    crowded_conditions = round(crowded_conditions * children_5_17), 
    no_computer_internet = round(no_computer_internet * children_5_17)
  )

```

### Statistical Disclosure Control Methods
In this section, we apply the current three most popular data privacy and confidentiality methods:

  1. **Suppression** - removing values
  2. **Synthetic Data** - replicating the confidential data based on a statistically representative model to generate pseudo or fake data records
  3. **Noise infusion** - adding noise using a differentially private method

#### Suppression
Suppression, or not reporting certain values from the data, is one of the earliest and simplest statistical disclosure control methods. For example, if two Black women live in Santa Fe County in New Mexico, then we would want to suppress or not report those people because they could be easily identified. The rules for determining the threshold for suppression vary based on data privacy laws, data type, and many other factors (Federal Committee on Statistical Methodology, 2005).

For our illustrative example, we suppress the marginal counts of the factors within each school districted if those values are below a threshold (e.g., if there are four students in a school district that are in poverty, then the value will be suppressed or removed if the threshold is five). We picked a threshold of 10 for this illustration. Some applications use lower values, such as 3 or 5, but we pick a higher value because the data we are treating as the confidential data has already been altered for privacy.

As mentioned earlier, an advantage of suppression is that the method is quick and simple to implement. Also, values above the selected threshold are unaltered, which may appeal to some users. A disadvantage is that no information is provided for the values at or below that selected threshold, which might be essential for some analyses.


```{r}
#| label: suppression
#| code-summary: "suppression"
#| message: false
#| warning: false

source("rcode/k_suppression.R")

k <- 10

# Suppression on New Mexico Data
nm_supp <- lapply(1:k, function(x) k_suppresion(nm_counts, x))

# Suppression on Pennsylvania Data
pa_supp <- lapply(1:k, function(x) k_suppresion(pa_counts, x))

```

#### Synthetic Data
Synthetic data consists of pseudo or “fake” records that are statistically representative of the confidential data. Records are considered synthesized when they are replaced with draws from a model fitted to the confidential data. Statisticians originally developed synthetic data as a conceptual extension of multiple imputation (Rubin 1993). Multiple imputation is used to address missing data in clinical trial scenarios and nonresponses in surveys by developing a model based on the remaining participants’ information to create new observations or values for the missing data. Synthetic data extends this concept to generating entirely new records for individuals to be released in place of the original records. The idea of synthetic data may be attractive to some because the synthetic data contain only “fake” records.

We generate fully synthetic data via a multinomial probability distribution with a Dirichlet prior to making our draws for each variable factor based on the total children count in the school district (Reiter et al. 2014). We use a Dirichlet prior distribution so we can vary the distributions parameters to adjust the “smoothness” of our synthetic data. The smoothing parameter values are 0.1, 0.25, and 0.5, where a large smoothing parameter means the draws will be made from more flat probabilities than the observed proportions. We expect this to impact smaller demographic groups than larger groups, as seen in Chapter 3. We have a postprocessing step (i.e., ensuring the results of the statistic or information are consistent with realistic constraints) to ensure that the total count is greater than or equal to max count of any factor. 

An advantage for synthetic data generation in our illustrative example is that we can report values for all school districts instead of removing some in the case of suppression. A drawback to this approach is that values are model dependent and could lead to improperly replicating the data for particular analyses. In our case, we first synthesize the total number of children count before we synthesize each variable individually as proportions. 


```{r}
#| label: synthetic
#| code-summary: "synthetic"
#| message: false
#| warning: false

# Synthetic data based on the proportions for each factor
source("rcode/synth_count.R")
source("rcode/post_processing.R")
source("rcode/synth.R")
source("rcode/synth_fun.R")

# Setting the seed  
set.seed(20230128)

# Code for parallel computing
cl <- makeCluster(detectCores() - 6)
clusterCall(cl, function() lapply(c("doParallel", "tidyverse"), require, character.only = TRUE)) %>% invisible()
registerDoParallel(cl)
set_urbn_defaults(style = "print")

clusterCall(cl, function() {source("rcode/synth_count.R")}) %>% invisible()
clusterCall(cl, function() {source("rcode/post_processing.R")}) %>% invisible()
clusterCall(cl, function() {source("rcode/synth.R")}) %>% invisible()
clusterCall(cl, function() {source("rcode/synth_fun.R")}) %>% invisible()

# Number of repetitions
n <- 20

# Dirichlet Prior tuning parameter
alpha <- c(0.1, 0.25, 0.5)

# Synthetic Data
set.seed(20230128)
nm_synth <- lapply(alpha, function(x) synth_fun(nm_counts, x, n))
pa_synth <- lapply(alpha, function(x) synth_fun(pa_counts, x, n))

stopCluster(cl)

```

#### Noise Infusion satisfying Differential Privacy
Another approach to protecting confidential data is adding random noise (i.e., adding or subtracting random values drawn from a probability distribution). One way to add noise or alter values in data is applying a method that satisfies differential privacy (DP) (Dwork et al. 2006). At a high level, DP is a strict mathematical definition that a method must satisfy (or meet the mathematical conditions) to be considered differentially private, not a statement or description of the data itself. Using methods that satisfy DP helps to quantify the relative privacy protections offered by adding different amounts of noise. 

A method that satisfies DP is the Laplace mechanism, which adds noise by drawing values from a Laplace distribution. The Laplace distribution is centered at zero and the distribution variability (i.e., wide or narrow the distribution is) is the ratio of the privacy loss budget, $\epsilon$, over the sensitivity of the target statistics. Having the distribution centered at zero means there is a higher probability of adding very little or no noise to the confidential data statistics. For the noise variability, if $\epsilon$ is large or the sensitivity of the statistic is low, then there is a higher probability of adding very little noise to confidential data statistic. If $\epsilon$ is small or the sensitivity of the statistics is high, then there is a higher probability of adding a lot of noise to the released statistic.

For our example, we will add Laplace noise to each count and normalize to the totals. In other words, we assume the total counts are invariant (i.e., no change to the statistics). We will test for $\epsilon$ = (0.01, 0.1, 1), which are values used in some real-world applications for tabular data sets. For example, the 2020 Decennial Census is a massive tabular data set of every person in the United States at various geographic levels and places people live (e.g., residential and dorms). They allocated roughly between 0.06 to 1.54 on various tabular statistics.
Similar to the synthetic data method, we have a postprocessing step to ensure that the total count is greater than or equal to max count of any factor. For this particular example, adding noise using the Laplace mechanism has similar advantages and disadvantages as the synthetic data generation method. The main difference is that DP methods formalize the privacy loss. 

```{r}
#| label: dp-laplace
#| code-summary: "laplace-mechanism"
#| message: false
#| warning: false

# Differentially private synthetic data based on the proportions for each factor.
source("rcode/dp_count.R")
source("rcode/helper_functions.R")
source("rcode/post_processing.R")
source("rcode/lap_san.R")
source("rcode/dp_fun.R")

set.seed(20230128)
n <- 10

eps <- c(0.01, 0.1, 1)

# Differentially private synthetic data
nm_dp <- lapply(eps, function(x) dp_fun(nm_counts, x, n))
pa_dp <- lapply(eps, function(x) dp_fun(pa_counts, x, n))

```

## Student Poverty
We imagine a state-level public policymaker might want to know the poverty levels of each school district to determine how much additional funding to provide for classroom supplies. In this example, we want to ensure the distribution of the share of students in poverty within each school district from the SDP method is similar to the confidential New Mexico and Pennsylvania data.

### New Mexico and Pennsylvania Poverty

```{r}
#| label: fig-scatter
#| fig-cap: "Comparing Poverty Rates across School Districts in New Mexico"
#| warning: false
#| message: false

# Creating the scatter plot for New Mexico
original <- nm_prop %>% 
  dplyr::select(poverty) %>%
  as_vector() %>%
  rep(length(eps)) %>%
  as_tibble()

poverty <- nm_dp[[1]] %>% 
    dplyr::select(poverty)
for(i in 2:length(eps)){
  temp <- nm_dp[[i]] %>% 
    dplyr::select(poverty)
  poverty <- bind_rows(poverty, temp)
}

n <- nrow(nm_prop)
dp <- c(rep("0.01", n), rep("0.1", n), rep("1", n))

nm_points_data <- bind_cols(original, poverty, dp)
names(nm_points_data) <- c("Original", "Poverty", "Privacy")

p1 <- ggplot(nm_points_data, aes(Original, Poverty)) +
  geom_abline() +
  geom_point(alpha = 0.5, aes(color = Privacy)) +
  coord_equal() +
  scale_x_continuous(limits = c(0, 1)) +
  scale_y_continuous(limits = c(0, 1)) +
  labs(
    title = "New Mexico",
    x = "Confidential Data Values",
    y = "Differentially Private Values",
    color = "Privacy Loss Budget"
  )

# Creating the scatter plot for Pennsylvania
original <- pa_prop %>% 
  dplyr::select(poverty) %>%
  as_vector() %>%
  rep(length(eps)) %>%
  as_tibble()

poverty <- pa_dp[[1]] %>% 
    dplyr::select(poverty)
for(i in 2:length(eps)){
  temp <- pa_dp[[i]] %>% 
    dplyr::select(poverty)
  poverty <- bind_rows(poverty, temp)
}

n <- nrow(pa_prop)
dp <- c(rep("0.01", n), rep("0.1", n), rep("1", n))

pa_points_data <- bind_cols(original, poverty, dp)
names(pa_points_data) <- c("Original", "Poverty", "Privacy")

p2 <- ggplot(pa_points_data, aes(Original, Poverty)) +
  geom_abline() +
  geom_point(alpha = 0.5, aes(color = Privacy)) +
  coord_equal() +
  scale_x_continuous(limits = c(0, 1)) +
  scale_y_continuous(limits = c(0, 1)) +
  labs(
    title = "Pennsylvania",
    x = "Confidential Data Values",
    y = "Differentially Private Values",
    color = "Privacy Loss Budget"
  )

p1 + p2

ggsave("figures/3-1.svg", width = 6.5, height = 4, units = "in")

# For generating the data to create these plots
write_csv(nm_points_data, "data/privacy_protected_data/3-1_nm-scatter.csv")
write_csv(pa_points_data, "data/privacy_protected_data/3-1_pa-scatter.csv")

```

## Correlation Matrix
We next investigate if there are any relationships between poverty and other factors. We first create a correlation matrix, a table that shows the statistical relationship between multiple factors, for New Mexico and Pennsylvania school districts using the confidential data, and then compare those results against the protected dataset. For this use case, we compare the confidential data to suppressed data, removing observations to make the data less sensitive. This comparison tells us how much the suppression affects our ability to discern relationships between poverty and other factors. To create the suppressed data, we remove the values from each factor from the original Urban report (e.g., linguistic isolation) within each school district if those values are below a threshold. If our threshold is five students, for example, then the values would be suppressed in a school district in which there are four students in poverty. Here, we picked a threshold of 10. 

### Original Data

#### New Mexico

```{r}
#| label: fig-nm-cor
#| fig-cap: "Correlations between Measures of Vulnerability across School Districts in New Mexico"
#| warning: false

# Correlation values
cor_nm <- nm_prop[, -c(1:2)] %>%
  cor()
cor_nm[upper.tri(cor_nm)] <- NA

# Correlation variable names
var <- c("Computer/Broadband", "Crowded Conditions", "Single Parent", "Vulnerable Economic Sectors", "Child Disability Status", "Linguistic Isolation", "Poverty")

# Correlation matrix
cor_nm %>%
  as_tibble(rownames = "var1") %>%
  pivot_longer(cols = -var1, names_to = "var2", values_to = "correlation") %>%
  mutate(
    var1 = factor(var1, levels = rev(colnames(cor_nm))),
    var2 = factor(var2, levels = colnames(cor_nm))
  ) %>%
  ggplot(aes(var1, var2, fill = correlation)) +
  geom_tile() + 
  geom_text(aes(label = round(correlation, 2)), size = 4) +
  scale_fill_gradientn(colours = palette_urbn_diverging, na.value = "white", limits = c(-1, 1)) +
  theme_minimal() + 
  theme(text = element_text(size = 15), axis.text.x = element_text(angle = 40, vjust = 1, hjust = 1)) +
  labs(x = NULL, y = NULL) + 
  scale_x_discrete(labels = var) + 
  scale_y_discrete(labels = rev(var)) + 
  labs(fill = "Correlation")

ggsave("figures/3-2.svg", width = 7, height = 5, units = "in")

# For generating the data to create the plots
temp_dat <- cor_nm %>%
  as_tibble(rownames = "var1") %>%
  pivot_longer(cols = -var1, names_to = "var2", values_to = "correlation")

write_csv(temp_dat, "data/privacy_protected_data/3-2_nm-heat.csv")

```

#### Pennsylvania

```{r}
#| label: fig-pa-cor
#| fig-cap: "Correlations between Measures of Vulnerability across School Districts in Pennsylvania"
#| warning: false

# Correlation values
cor_pa <- pa_prop[, -(1:2)] %>%
  cor()
cor_pa[upper.tri(cor_pa)] <- NA

# Correlation matrix
cor_pa %>%
  as_tibble(rownames = "var1") %>%
  pivot_longer(cols = -var1, names_to = "var2", values_to = "correlation") %>%
  mutate(
    var1 = factor(var1, levels = rev(colnames(cor_pa))),
    var2 = factor(var2, levels = colnames(cor_pa))
  ) %>%
  ggplot(aes(var1, var2, fill = correlation)) +
  geom_tile() + 
  geom_text(aes(label = round(correlation, 2)), size = 4) +
  scale_fill_gradientn(colours = palette_urbn_diverging, na.value = "white", limits = c(-1, 1)) +
  theme_minimal() + 
  theme(text = element_text(size = 15), axis.text.x = element_text(angle = 40, vjust = 1, hjust = 1)) +
  labs(x = NULL, y = NULL) + 
  scale_x_discrete(labels = var) + 
  scale_y_discrete(labels = rev(var)) + 
  labs(fill = "Correlation")

ggsave("figures/3-4.svg", width = 7, height = 5, units = "in")

# For generating the data to create the plots
temp_dat <- cor_pa %>%
  as_tibble(rownames = "var1") %>%
  pivot_longer(cols = -var1, names_to = "var2", values_to = "correlation")

write_csv(temp_dat, "data/privacy_protected_data/3-4_pa-heat.csv")
```

### Suppressed Data

#### New Mexico
For the New Mexico suppressed data, we see that "lack of computer or broadband access" is still the most correlated (0.61) with "single parent" is next (0.44).

```{r}
#| label: fig-cor-suppressed-nm
#| fig-cap: "Correlations between Measures of Vulnerability across School Districts in New Mexico (Suppressed, k = 10)"
#| warning: false

# Correlation values New Mexico

# Plotting for k = 10
k <- 10

cor_pub <- nm_supp[[k]][, -(1:2)] %>%
  cor(use = "complete.obs")
cor_pub[upper.tri(cor_pub)] <- NA

bias <- cor_pub - cor_nm

bias %>% abs() %>% mean(na.rm = TRUE) %>% round(digits = 2)

# Correlation matrix for suppressed data
p1 <- cor_pub %>%
  as_tibble(rownames = "var1") %>%
  pivot_longer(cols = -var1, names_to = "var2", values_to = "correlation") %>%
  mutate(
    var1 = factor(var1, levels = rev(colnames(cor_pub))),
    var2 = factor(var2, levels = colnames(cor_pub))
  ) %>%
  ggplot(aes(var1, var2, fill = correlation)) +
  geom_tile() + 
  geom_text(aes(label = round(correlation, 2)), size = 4) +
  scale_fill_gradientn(colours = palette_urbn_diverging, na.value = "white", limits = c(-1, 1)) +
  theme_minimal() + 
  theme(text = element_text(size = 15), axis.text.x = element_text(angle = 40, vjust = 1, hjust = 1)) +
  labs(x = NULL, y = NULL) + 
  scale_x_discrete(labels = var) + 
  scale_y_discrete(labels = rev(var)) + 
  labs(fill = "Correlation")

# Correlation matrix for suppressed data - original data
p2 <- bias %>%
  as_tibble(rownames = "var1") %>%
  pivot_longer(cols = -var1, names_to = "var2", values_to = "correlation") %>%
  mutate(
    var1 = factor(var1, levels = rev(colnames(bias))),
    var2 = factor(var2, levels = colnames(bias))
  ) %>%
  ggplot(aes(var1, var2, fill = correlation)) +
  geom_tile() + 
  geom_text(aes(label = round(correlation, 2)), size = 4) +
  scale_fill_gradientn(colours = palette_urbn_diverging, na.value = "white", limits = c(-1, 1)) +
  theme_minimal() + 
  theme(text = element_text(size = 15), axis.text.x = element_text(angle = 40, vjust = 1, hjust = 1)) +
  labs(x = NULL, y = NULL) + 
  scale_x_discrete(labels = var) + 
  scale_y_discrete(labels = rev(var)) + 
  labs(fill = "Correlation Bias")

p1 + p2

ggsave("figures/3-3.svg", width = 14.5, height = 5, units = "in")

# For generating the data to create the plots
temp_dat <- cor_pub %>%
  as_tibble(rownames = "var1") %>%
  pivot_longer(cols = -var1, names_to = "var2", values_to = "correlation")

write_csv(temp_dat, "data/privacy_protected_data/3-3_nm_heat_suppress.csv")

temp_dat <- bias %>%
  as_tibble(rownames = "var1") %>%
  pivot_longer(cols = -var1, names_to = "var2", values_to = "correlation")

write_csv(temp_dat, "data/privacy_protected_data/3-3_nm-heat-suppress-bias.csv")

```

#### Pennsylvania

```{r}
#| label: fig-cor-suppressed-pa
#| fig-cap: "Correlations between Measures of Vulnerability across School Districts in Pennsylvania (Suppressed, k = 10)"
#| fig-subcap:
#|   - "Correlations (Suppressed, k = 10)"
#|   - "Correlations Bias" 
#| warning: false

# Correlation values Pennsylvania
cor_pub <- pa_supp[[k]][, -(1:2)] %>%
  cor(use = "complete.obs")
cor_pub[upper.tri(cor_pub)] <- NA

bias <- cor_pub - cor_pa

bias %>% abs() %>% mean(na.rm = TRUE) %>% round(digits = 2)

# Correlation matrix for suppressed data
p1 <- cor_pub %>%
  as_tibble(rownames = "var1") %>%
  pivot_longer(cols = -var1, names_to = "var2", values_to = "correlation") %>%
  mutate(
    var1 = factor(var1, levels = rev(colnames(cor_pub))),
    var2 = factor(var2, levels = colnames(cor_pub))
  ) %>%
  ggplot(aes(var1, var2, fill = correlation)) +
  geom_tile() + 
  geom_text(aes(label = round(correlation, 2)), size = 4) +
  scale_fill_gradientn(colours = palette_urbn_diverging, na.value = "white", limits = c(-1, 1)) +
  theme_minimal() + 
  theme(text = element_text(size = 15), axis.text.x = element_text(angle = 40, vjust = 1, hjust = 1)) +
  labs(x = NULL, y = NULL) + 
  scale_x_discrete(labels = var) + 
  scale_y_discrete(labels = rev(var)) + 
  labs(fill = "Correlation")

# Correlation matrix for suppressed data - original data
p2 <- bias %>%
  as_tibble(rownames = "var1") %>%
  pivot_longer(cols = -var1, names_to = "var2", values_to = "correlation") %>%
  mutate(
    var1 = factor(var1, levels = rev(colnames(bias))),
    var2 = factor(var2, levels = colnames(bias))
  ) %>%
  ggplot(aes(var1, var2, fill = correlation)) +
  geom_tile() + 
  geom_text(aes(label = round(correlation, 2)), size = 4) +
  scale_fill_gradientn(colours = palette_urbn_diverging, na.value = "white", limits = c(-1, 1)) +
  theme_minimal() + 
  theme(text = element_text(size = 15), axis.text.x = element_text(angle = 40, vjust = 1, hjust = 1)) +
  labs(x = NULL, y = NULL) + 
  scale_x_discrete(labels = var) + 
  scale_y_discrete(labels = rev(var)) + 
  labs(fill = "Correlation Bias")

p1 + p2

ggsave("figures/3-5.svg", width = 15, height = 5, units = "in")

# For generating the data to create the plots
temp_dat <- cor_pub %>%
  as_tibble(rownames = "var1") %>%
  pivot_longer(cols = -var1, names_to = "var2", values_to = "correlation")

write_csv(temp_dat, "data/privacy_protected_data/3-5_pa_heat_suppress.csv")

temp_dat <- bias %>%
  as_tibble(rownames = "var1") %>%
  pivot_longer(cols = -var1, names_to = "var2", values_to = "correlation")

write_csv(temp_dat, "data/privacy_protected_data/3-5_pa-heat-suppress-bias.csv")
```

#### Privacy-Utility Curve

```{r}
#| label: privacy-utility
#| warning: false

# Correlation values New Mexico
nm_bias <- vector()
for(i in 1:k) {
  cor_pub <- nm_supp[[i]][, -(1:2)] %>%
  cor(use = "complete.obs")
  
  cor_pub[upper.tri(cor_pub)] <- NA

  nm_bias[i] <- (cor_pub - cor_nm) %>% abs() %>% mean(na.rm = TRUE) %>% round(digits = 3)
}


# Correlation values Pennsylvania
pa_bias <- vector()
for(i in 1:k) {
  cor_pub <- pa_supp[[i]][, -(1:2)] %>%
  cor(use = "complete.obs")
  
  cor_pub[upper.tri(cor_pub)] <- NA

  pa_bias[i] <- (cor_pub - cor_pa) %>% abs() %>% mean(na.rm = TRUE) %>% round(digits = 3)
}

# Line Plot

nm_cor <- bind_cols(K = 1:k, Bias = nm_bias, State = "New Mexico")

pa_cor <- bind_cols(K = 1:k, Bias = pa_bias, State = "Pennsylvania")

cor_data <- bind_rows(nm_cor, pa_cor) %>%
  mutate(State = fct_relevel(State, 
            "New Mexico", "Pennsylvania"))

ggplot(cor_data, aes(x = K, y = Bias, color = State)) + 
  geom_line() +
  scale_x_continuous(limits = c(1, k),
                     breaks = 1:k) +
  scale_y_continuous(limits = c(0, 0.08)) +
  scale_y_reverse() +
  scale_x_reverse() +
  labs(x = "Suppression Threshold",
       y = "Mean of the Absolute Correlation Bias") + 
  theme(text = element_text(size = 15))

# For generating the data to create the plots

write_csv(cor_data, "data/privacy_protected_data/3-6_privacy-utility-curve.csv")
```

## Broadband Access and Single Parent
For our final example, we examine the factors most correlated with poverty for these two states. In New Mexico, we examine the share of students without access to a computer or broadband internet and in Pennsylvania, the share of students with a single parent. 

### Original Data

#### New Mexico
```{r}
#| label: map-data-nm
#| code-summary: "map-nm"
#| message: false
#| warning: false

# School District Files
schools_nm <- school_districts("New Mexico") %>% invisible()

# Creating the maps
set_urbn_defaults(style = "map")

data_broadband <- nm_prop %>%
    dplyr::select(
      geographic_school_district,
      no_computer_internet
    )

schools_nm <- rename(schools_nm, geographic_school_district = NAME) %>% invisible()

temp <- schools_nm %>%
    dplyr::select(
      geographic_school_district,
      INTPTLAT,
      INTPTLON,
      geometry
    ) %>%
  right_join(data_broadband, by = "geographic_school_district")

ggplot() +
  geom_sf(temp,
          mapping = aes(fill = no_computer_internet),
          color = "#ffffff", size = 0.25) +
  scale_fill_gradientn(labels = scales::percent, limits = c(0, 1)) + 
  labs(fill = "Computer/Broadband")

ggsave("figures/3-7.svg", width = 5, height = 5, units = "in")

# For generating the data to create the plots
write_csv(temp, "data/privacy_protected_data/3-7_nm-broad-band.csv")

```

#### Pennsylvania
```{r}
#| label: map-data-pa
#| code-summary: "map-pa"
#| message: false
#| warning: false

# School District Files
schools_pa <- school_districts("Pennsylvania")

# Creating the maps
set_urbn_defaults(style = "map")

pa_proprent <- pa_prop %>%
    dplyr::select(
      geographic_school_district,
      single_parent
    )

schools_pa <- rename(schools_pa, geographic_school_district = NAME)

temp <- schools_pa %>%
    dplyr::select(
      geographic_school_district,
      INTPTLAT,
      INTPTLON,
      geometry
    ) %>%
  right_join(pa_proprent, by = "geographic_school_district")

ggplot() +
  geom_sf(temp,
          mapping = aes(fill = single_parent),
          color = "#ffffff", size = 0.25) +
  scale_fill_gradientn(labels = scales::percent, limits = c(0, 1)) + 
  labs(fill = "Single Parent")

ggsave("figures/3-9.svg", width = 6.5, height = 4, units = "in")

# For generating the data to create the plots
write_csv(temp, "data/privacy_protected_data/3-9_pa-single-parent.csv")

```

### Synthetic Data

#### New Mexico

```{r}
#| label: fig-map-nm-synth
#| fig-cap: "Lack of Computer or Broadband Access in New Mexico (Synthetic Data)"
#| message: false
#| warning: false

source("rcode/broadband_fun.R")

bias <- broadband_fun(nm_synth[[1]], schools_nm, nm_prop)
bias2 <- broadband_fun(nm_synth[[2]], schools_nm, nm_prop)
bias3 <- broadband_fun(nm_synth[[3]], schools_nm, nm_prop)

bias <- bind_rows(bias, bias2, bias3)

n <- nrow(nm_prop)
alpha <- c(0.1, 0.25, 0.5)
alpha <- rep(alpha, each = n)

bias <- bind_cols(bias, alpha)
colnames(bias) <- c("geographic_school_district", "INTPTLAT", "INTPTLON", "no_computer_internet", "Alpha", "geometry")

ggplot() +
  geom_sf(bias,
          mapping = aes(fill = no_computer_internet),
          color = "#ffffff", size = 0.25) +
  scale_fill_gradientn(colors = palette_urbn_diverging, labels = scales::percent, limits = c(-0.25, 0.25)) + 
  labs(fill = "Computer/Broadband Bias") +
    facet_wrap(. ~ Alpha)


ggsave("figures/3-8.svg", width = 6.5, height = 4, units = "in")

# For generating the data to create the plots
write_csv(temp, "data/privacy_protected_data/3-8_nm-broad-band-synthetic.csv")
  
```

#### Pennsylvania

```{r}
#| label: fig-map-pa-synth
#| fig-cap: "Single Parent in Pennsylvania (Synthetic Data)"
#| message: false
#| warning: false

source("rcode/parent_fun.R")

bias <- parent_fun(pa_synth[[1]], schools_pa, pa_prop)
bias2 <- parent_fun(pa_synth[[2]], schools_pa, pa_prop)
bias3 <- parent_fun(pa_synth[[3]], schools_pa, pa_prop)

bias <- bind_rows(bias, bias2, bias3)

n <- nrow(pa_prop)
alpha <- c(0.1, 0.25, 0.5)
alpha <- rep(alpha, each = n)

bias <- bind_cols(bias, alpha)
colnames(bias) <- c("geographic_school_district", "INTPTLAT", "INTPTLON", "single_parent", "Alpha", "geometry")

ggplot() +
  geom_sf(bias,
          mapping = aes(fill = single_parent),
          color = "#ffffff", size = 0.25) +
  scale_fill_gradientn(colors = palette_urbn_diverging, labels = scales::percent, limits = c(-0.25, 0.25)) + 
  labs(fill = "Single Parent Bias") +
    facet_wrap(. ~ Alpha, dir = "h")

ggsave("figures/3-10.svg", width = 6.5, height = 4, units = "in")

# For generating the data to create the plots
write_csv(temp, "data/privacy_protected_data/3-10_pa-broad-band-synthetic.csv")

```