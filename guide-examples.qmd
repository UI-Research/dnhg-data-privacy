---
title: "Do No Harm Guide"
subtitle: "Applying Equity Awareness in Data Privacy Methods Examples"
author-title: "Authors"
authors: "Claire McKay Bowen and Joshua Snoke"
affiliation: "Urban Institute and RAND Corp."
date: "today"
format:
  html:
    theme: urbn.scss
    self-contained: true
    code-fold: true
    code-line-numbers: true
    html-math-method: katex
    df-print: default
    toc: true
    toc-depth: 3
    number-sections: true
    number-depth: 3
    highlight-style: pygments
editor_options: 
  chunk_output_type: console
execute: 
  echo: false
---

This Quarto Document walks through "Do No Harm Guide: Applying Equity Awareness in Data Privacy Methods".

**Disclaimer:** These analyses are inspired by real data and public policy analytics, but are *not* representative of them.

```{r}
#| label: load-pkgs
#| code-summary: "Packages"
#| message: false
#| warning: false
#| echo: false
library(tidyverse)  # for data wrangling and visualization
library(knitr)      # for tables
library(readxl)     # for importing excel (data)
library(smoothmest) # for laplace distribution
library(tigris)     # for census shape files
library(patchwork)  # for combining ggplots
library(urbnthemes) # for ggplot2 theme
library(doParallel) # for parallel computing
```

## Data

Our illustrative example comes from an Urban Institute research report called, "[Mapping Student Needs during COVID-19](https://www.urban.org/research/publication/mapping-student-needs-during-covid-19)" and can be accessed at [Urban Data Catalog](https://datacatalog.urban.org/dataset/household-conditions-geographic-school-district). The purpose of the research is to “…highlight different types of challenges to remote learning and point to district and educator strategies that mitigate harm to students as districts navigate long-term school closures.” In addition to poverty, the researchers examined six other factors: linguistic isolation, child disability status, parents in vulnerable economic sectors, single parent, crowded conditions, and lack of computer or broadband access. See report for how they define these factors.

### Confidential Data
The researchers used 2014 through 2018 data from the American Community Survey  to conduct their analyses. The researchers used two versions of the American Community Survey, but we focus on one and treat the data as the confidential data. We define the confidential data as the cleaned version (meaning edited for inaccuracies or inconsistencies). Researchers often refer to this version of the data as the gold standard or actual data for analysis. The published American Community Survey data are public data that have undergone SDP protections prior to being published, but we treat this data as the confidential data.

The data set is the 2014 through 2018 five-year estimate from the National Historical Geographic Information System. The National Historical Geographic Information System data has the seven variables of interest as proportions with margins of error. For simplicity, we assume the proportions are the true values and create another version of the data into tabular values based on those proportions. For this guide, we highlight two states, New Mexico and Pennsylvania. We pick these states because they vary greatly in population density and the proportion of the seven variables, which demonstrates how population density affect statistical data privacy results.

**Disclaimer:** Please note that the analyses in this chapter are inspired by real data and public policy analytics but are not representative of them.

```{r}
#| label: data
#| code-summary: "data"
#| message: false
#| warning: false

# Proportions
nm_prop <- read_excel("data/data/nhgis_district_data_var.xlsx") %>%
  filter(state == "New Mexico") %>%
  dplyr::select(
    geographic_school_district,
    children_5_17,
    poverty,
    linguistically_isolated,
    children_disability,
    vulnerable_job,
    single_parent,
    crowded_conditions, 
    no_computer_internet
  )

pa_prop <- read_excel("data/data/nhgis_district_data_var.xlsx") %>%
  filter(state == "Pennsylvania") %>%
  dplyr::select(
    geographic_school_district,
    children_5_17,
    poverty,
    linguistically_isolated,
    children_disability,
    vulnerable_job,
    single_parent,
    crowded_conditions, 
    no_computer_internet
  ) %>%
  head(-1) # Empty district

# Tabular Data
nm_counts <- nm_prop %>%
  mutate(
    poverty = round(poverty * children_5_17),
    linguistically_isolated = round(linguistically_isolated * children_5_17),
    children_disability = round(children_disability * children_5_17),
    vulnerable_job = round(vulnerable_job * children_5_17),
    single_parent = round(single_parent * children_5_17),
    crowded_conditions = round(crowded_conditions * children_5_17), 
    no_computer_internet = round(no_computer_internet * children_5_17)
  )

pa_counts <- pa_prop %>%
  mutate(
    poverty = round(poverty * children_5_17),
    linguistically_isolated = round(linguistically_isolated * children_5_17),
    children_disability = round(children_disability * children_5_17),
    vulnerable_job = round(vulnerable_job * children_5_17),
    single_parent = round(single_parent * children_5_17),
    crowded_conditions = round(crowded_conditions * children_5_17), 
    no_computer_internet = round(no_computer_internet * children_5_17)
  )

```

### Statistical Disclosure Control Methods
In this section, we apply the current three most popular data privacy and confidentiality methods:

  1. **Suppression** - removing values
  2. **Synthetic Data** - replicating the confidential data based on a statistically representative model to generate pseudo or fake data records
  3. **Noise infusion** - adding noise using a differentially private method

#### Suppression
Suppression, or not reporting certain values from the data, is one of the earliest and easiest statistical disclosure control methods. For example, if two African American women live in Santa Fe County in New Mexico, then we would want to suppress or not report those people because they could be easily identified. The rules for determining the threshold for suppression vary based on data privacy laws, data type, and many other factors.

A common version of suppression is to suppress the counts of values if they do not meet a threshold. For example, if our threshold is 3 and there

For our illustrative example, we will suppress the data up to k = 10 for each of the seven factors. Typical applications use lower values of k, such as k = 3 or 5, but we pick a higher value because the data we are treating as the confidential data has already been altered for privacy.

As mentioned earlier, an advantage of suppression is the method is quick and simple to implement. Also, values above k are unaltered, improving utility for those values. A disadvantage is that no information is provided for the values at or below k, which might be essential for some analyses.

```{r}
#| label: suppression
#| code-summary: "suppression"
#| message: false
#| warning: false

source("rcode/k_suppression.R")

k <- 10

# Suppression on New Mexico Data
nm_supp <- lapply(1:k, function(x) k_suppresion(nm_counts, x))

# Suppression on Pennsylvania Data
pa_supp <- lapply(1:k, function(x) k_suppresion(pa_counts, x))

```

#### Synthetic Data
Synthetic data consists of pseudo or “fake” records that are statistically representative of the confidential data. Records are considered synthesized when they are replaced with draws from a model fitted to the confidential data. Statisticians originally developed synthetic data as a conceptually extension of multiple imputation (Rubin 1993). Multiple imputation is used to address missing data in clinical trial scenarios and nonresponses in surveys by developing a model based on the remaining participants’ information to create new observations or values for the missing data. Synthetic data extends this concept to generating entirely new records for individuals to be released in place of the original records. The idea of synthetic data is attractive to several federal agencies because the synthetic data contain only “fake” records.
There are generally two ways of generating synthetic data: parametric and nonparametric.

Parametric models assume a finite number of parameters that capture the complexity of the data. They are generally less flexible, but more interpretable than nonparametric models. Some examples include using a regression model to assign an age variable, sampling from a probability distribution, Bayesian models, copula-based models.

Nonparametric data synthesis is the process of data generation that is not based on assumptions about an underlying distribution or model. Nonparametric methods often use frequency proportions or marginal probabilities as weights for some type of sampling scheme. They are generally more flexible, but less interpretable than parametric models. One example is assigning gender based on underlying proportions. Another example is applying a classification and regression trees or more commonly known as CART model.

We will generate fully synthetic data via a nonparametric model (Reiter 2005; Drechsler and Reiter 2011) for each factor and total children count. We have a postprocessing step (i.e., ensuring the results of the statistic or information are consistent with realistic constraints) to ensure that the total count is greater than or equal to max count of any factor. We also assume zeros (i.e., if there is a zero in the data, we do not alter it).

An advantage for synthetic data generation in our illustrative example is that we can report values for all school districts instead of removing some in the case of suppression. A drawback to this approach is that values are model dependent and could lead to improperly replicating the data for particular analyses. In our case, we first synthesize the total children count before we synthesize each variable individually as proportions. 

```{r}
#| label: synthetic
#| code-summary: "synthetic"
#| message: false
#| warning: false

# Synthetic data based on the proportions for each factor
source("rcode/synth_count.R")
source("rcode/post_processing.R")
source("rcode/synth.R")
source("rcode/synth_fun.R")

# Setting the seed  
set.seed(20230128)

# Code for parallel computing
cl <- makeCluster(detectCores() - 6)
clusterCall(cl, function() lapply(c("doParallel", "tidyverse"), require, character.only = TRUE)) %>% invisible()
registerDoParallel(cl)
set_urbn_defaults(style = "print")

clusterCall(cl, function() {source("rcode/synth_count.R")}) %>% invisible()
clusterCall(cl, function() {source("rcode/post_processing.R")}) %>% invisible()
clusterCall(cl, function() {source("rcode/synth.R")}) %>% invisible()
clusterCall(cl, function() {source("rcode/synth_fun.R")}) %>% invisible()

# Number of repetitions
n <- 20

# Dirichlet Prior tuning parameter
alpha <- c(0.1, 0.25, 0.5)

# Synthetic Data
nm_synth <- lapply(alpha, function(x) synth_fun(nm_counts, x, n))
pa_synth <- lapply(alpha, function(x) synth_fun(pa_counts, x, n))

stopCluster(cl)

```

#### DP synthetic - Laplace sanitizer
Another approach to protect confidential data is adding random noise (i.e., adding or subtracting random values drawn from a probability distribution). One way to add noise or alter values in data is applying a method that satisfies differential privacy (DP) (Dwork et al. 2006). At a high level, DP is a strict mathematical definition that a method must satisfy (or meet the mathematical conditions) to be considered differentially private, not a statement or description of the data itself. Using methods that satisfy DP helps to quantify the relative privacy protections offered by adding different amounts of noise. 

A method that satisfies DP is the Laplace mechanism, which adds noise by drawing values from a Laplace distribution. The Laplace distribution is centered at zero and the distribution variability (i.e., wide or narrow the distribution is) is the ratio of the privacy loss budget, ϵ, over the sensitivity of the target statistics. Having the distribution centered at zero means there is a higher probability of adding very little or no noise to the confidential data statistics. For the noise variability, if $\epsilon$ is large or the sensitivity of the statistic is low, then there is a higher probability of adding very little noise to confidential data statistic. If $\epsilon$ is small or the sensitivity of the statistics is high, then there is a higher probability of adding a lot of noise to the released statistic.

For our example, we will add Laplace noise to each count and normalize to the totals. In other words, we assume the total counts are invariant (i.e., no change to the statistics). We will test for $\epsilon$ = 0.01, which is a value comparable to some real-world applications for tabular data sets, such as the 2020 Decennial Census.

Similar to the synthetic data method, we have a postprocessing step to ensure that the total count is greater than or equal to max count of any factor. We also assume zeros.

DP methods that create synthetic data have similar advantages and disadvantages as synthetic data generation methods. The main difference is that DP methods can account for privacy loss. 

```{r}
#| label: dp-laplace
#| code-summary: "laplace-mechanism"
#| message: false
#| warning: false

# Differentially private synthetic data based on the proportions for each factor.
source("rcode/dp_count.R")
source("rcode/helper_functions.R")
source("rcode/post_processing.R")
source("rcode/lap_san.R")
source("rcode/dp_fun.R")

set.seed(20230128)
n <- 10

eps <- c(0.01, 0.1, 1)

# Differentially private synthetic data
nm_dp <- lapply(eps, function(x) dp_fun(nm_counts, x, n))
pa_dp <- lapply(eps, function(x) dp_fun(pa_counts, x, n))

```

## Scatter Plots
We first look at how the values for each factor are distributed. Specifically, we examine the distribution of the share of students in poverty since the Urban report looked at poverty levels.

### New Mexico and Pennsylvania Poverty

```{r}
#| label: fig-scatter
#| fig-cap: "Comparing Poverty Rates across School Districts in New Mexico"
#| warning: false
#| message: false

# Creating the scatter plot for New Mexico
original <- nm_prop %>% 
  dplyr::select(poverty) %>%
  as_vector() %>%
  rep(length(eps)) %>%
  as_tibble()

poverty <- nm_dp[[1]] %>% 
    dplyr::select(poverty)
for(i in 2:length(eps)){
  temp <- nm_dp[[i]] %>% 
    dplyr::select(poverty)
  poverty <- bind_rows(poverty, temp)
}

n <- nrow(nm_prop)
dp <- c(rep("0.01", n), rep("0.1", n), rep("1", n))

points_data <- bind_cols(original, poverty, dp)
names(points_data) <- c("Original", "Poverty", "Privacy")

p1 <- ggplot(points_data, aes(Original, Poverty)) +
  geom_abline() +
  geom_point(alpha = 0.5, aes(color = Privacy)) +
  coord_equal() +
  scale_x_continuous(limits = c(0, 1)) +
  scale_y_continuous(limits = c(0, 1)) +
  labs(
    title = "New Mexico",
    x = "Confidential Data Values",
    y = "Differentially Private Values",
    color = "Privacy Loss Budget"
  )

# Creating the scatter plot for Pennsylvania
original <- pa_prop %>% 
  dplyr::select(poverty) %>%
  as_vector() %>%
  rep(length(eps)) %>%
  as_tibble()

poverty <- pa_dp[[1]] %>% 
    dplyr::select(poverty)
for(i in 2:length(eps)){
  temp <- pa_dp[[i]] %>% 
    dplyr::select(poverty)
  poverty <- bind_rows(poverty, temp)
}

n <- nrow(pa_prop)
dp <- c(rep("0.01", n), rep("0.1", n), rep("1", n))

points_data <- bind_cols(original, poverty, dp)
names(points_data) <- c("Original", "Poverty", "Privacy")

p2 <- ggplot(points_data, aes(Original, Poverty)) +
  geom_abline() +
  geom_point(alpha = 0.5, aes(color = Privacy)) +
  coord_equal() +
  scale_x_continuous(limits = c(0, 1)) +
  scale_y_continuous(limits = c(0, 1)) +
  labs(
    title = "Pennsylvania",
    x = "Confidential Data Values",
    y = "Differentially Private Values",
    color = "Privacy Loss Budget"
  )

p1 + p2

```

### Pennsylvania Poverty

```{r}
#| label: fig-pa-line
#| fig-cap: "Comparing Poverty Rates across School Districts in Pennsylvania"
#| warning: false
#| message: false


```

## Correlation Matrix
Our next utility metric is creating a correlation matrix. The Urban report focused on what factors were highly correlated with poverty. We repeat this analysis for our New Mexico and Pennsylvania.

### Original Data

#### New Mexico
For New Mexico, we see that "lack of computer or broadband access" is most correlated (0.53) with "single parent" is next (0.39). The other factors would be considered weak, where the values are under 0.15 (the next largest at 0.12).

```{r}
#| label: fig-nm-cor
#| fig-cap: "Correlations between Measures of Vulnerability across School Districts in New Mexico"
#| warning: false

# Correlation values
cor_nm <- nm_prop[, -c(1:2)] %>%
  cor()
cor_nm[upper.tri(cor_nm)] <- NA

# Correlation variable names
var <- c("Computer/Broadband", "Crowded Conditions", "Single Parent", "Vulnerable Economic Sectors", "Child Disability Status", "Linguistic Isolation", "Poverty")

# Correlation matrix
cor_nm %>%
  as_tibble(rownames = "var1") %>%
  pivot_longer(cols = -var1, names_to = "var2", values_to = "correlation") %>%
  mutate(
    var1 = factor(var1, levels = rev(colnames(cor_nm))),
    var2 = factor(var2, levels = colnames(cor_nm))
  ) %>%
  ggplot(aes(var1, var2, fill = correlation)) +
  geom_tile() + 
  geom_text(aes(label = round(correlation, 2)), size = 4) +
  scale_fill_gradientn(colours = palette_urbn_diverging, na.value = "white", limits = c(-1, 1)) +
  theme_minimal() + 
  theme(text = element_text(size = 15), axis.text.x = element_text(angle = 40, vjust = 1, hjust = 1)) +
  labs(x = NULL, y = NULL) + 
  scale_x_discrete(labels = var) + 
  scale_y_discrete(labels = rev(var)) + 
  labs(fill = "Correlation")
```

#### Pennsylvania
For Pennsylvania, we see that "single parent" is most correlated (0.71) with "child disability status" and "lack of computer or broadband access" next (0.53 and 0.48, respectively).

```{r}
#| label: fig-pa-cor
#| fig-cap: "Correlations between Measures of Vulnerability across School Districts in Pennsylvania"
#| warning: false

# Correlation values
cor_pa <- pa_prop[, -(1:2)] %>%
  cor()
cor_pa[upper.tri(cor_pa)] <- NA

# Correlation matrix
cor_pa %>%
  as_tibble(rownames = "var1") %>%
  pivot_longer(cols = -var1, names_to = "var2", values_to = "correlation") %>%
  mutate(
    var1 = factor(var1, levels = rev(colnames(cor_pa))),
    var2 = factor(var2, levels = colnames(cor_pa))
  ) %>%
  ggplot(aes(var1, var2, fill = correlation)) +
  geom_tile() + 
  geom_text(aes(label = round(correlation, 2)), size = 4) +
  scale_fill_gradientn(colours = palette_urbn_diverging, na.value = "white", limits = c(-1, 1)) +
  theme_minimal() + 
  theme(text = element_text(size = 15), axis.text.x = element_text(angle = 40, vjust = 1, hjust = 1)) +
  labs(x = NULL, y = NULL) + 
  scale_x_discrete(labels = var) + 
  scale_y_discrete(labels = rev(var)) + 
  labs(fill = "Correlation")
```

### Suppressed Data

#### New Mexico
For the New Mexico suppressed data, we see that "lack of computer or broadband access" is still the most correlated (0.61) with "single parent" is next (0.44).

```{r}
#| label: fig-cor-suppressed-nm
#| fig-cap: "Correlations between Measures of Vulnerability across School Districts in New Mexico (Suppressed, k = 10)"
#| warning: false

# Correlation values New Mexico

# Plotting for k = 10
k <- 10

cor_pub <- nm_supp[[k]][, -(1:2)] %>%
  cor(use = "complete.obs")
cor_pub[upper.tri(cor_pub)] <- NA

bias <- cor_pub - cor_nm

bias %>% abs() %>% mean(na.rm = TRUE) %>% round(digits = 2)

# Correlation matrix for suppressed data
p1 <- cor_pub %>%
  as_tibble(rownames = "var1") %>%
  pivot_longer(cols = -var1, names_to = "var2", values_to = "correlation") %>%
  mutate(
    var1 = factor(var1, levels = rev(colnames(cor_pub))),
    var2 = factor(var2, levels = colnames(cor_pub))
  ) %>%
  ggplot(aes(var1, var2, fill = correlation)) +
  geom_tile() + 
  geom_text(aes(label = round(correlation, 2)), size = 5) +
  scale_fill_gradientn(colours = palette_urbn_diverging, na.value = "white", limits = c(-1, 1)) +
  theme_minimal() + 
  theme(text = element_text(size = 20), axis.text.x = element_text(angle = 40, vjust = 1, hjust = 1)) +
  labs(x = NULL, y = NULL) + 
  scale_x_discrete(labels = var) + 
  scale_y_discrete(labels = rev(var)) + 
  labs(fill = "Correlation")

# Correlation matrix for suppressed data - original data
p2 <- bias %>%
  as_tibble(rownames = "var1") %>%
  pivot_longer(cols = -var1, names_to = "var2", values_to = "correlation") %>%
  mutate(
    var1 = factor(var1, levels = rev(colnames(bias))),
    var2 = factor(var2, levels = colnames(bias))
  ) %>%
  ggplot(aes(var1, var2, fill = correlation)) +
  geom_tile() + 
  geom_text(aes(label = round(correlation, 2)), size = 5) +
  scale_fill_gradientn(colours = palette_urbn_diverging, na.value = "white", limits = c(-1, 1)) +
  theme_minimal() + 
  theme(text = element_text(size = 20), axis.text.x = element_text(angle = 40, vjust = 1, hjust = 1)) +
  labs(x = NULL, y = NULL) + 
  scale_x_discrete(labels = var) + 
  scale_y_discrete(labels = rev(var)) + 
  labs(fill = "Correlation Bias")

p1 + p2

```

#### Pennsylvania
For the Pennsylvania suppressed data, we see that "single parent" is still the most correlated (0.71) with "children with disability" and "lack of computer or broadband access" next (0.52 and 0.48, respectively). These values are almost exactly the same as the original data, which is expected because Pennsylvania has a fairly dense population.

```{r}
#| label: fig-cor-suppressed-pa
#| fig-cap: "Correlations between Measures of Vulnerability across School Districts in Pennsylvania (Suppressed, k = 10)"
#| fig-subcap:
#|   - "Correlations (Suppressed, k = 10)"
#|   - "Correlations Bias" 
#| warning: false

# Correlation values Pennsylvania
cor_pub <- pa_supp[[k]][, -(1:2)] %>%
  cor(use = "complete.obs")
cor_pub[upper.tri(cor_pub)] <- NA

bias <- cor_pub - cor_pa

bias %>% abs() %>% mean(na.rm = TRUE) %>% round(digits = 2)

# Correlation matrix for suppressed data
p1 <- cor_pub %>%
  as_tibble(rownames = "var1") %>%
  pivot_longer(cols = -var1, names_to = "var2", values_to = "correlation") %>%
  mutate(
    var1 = factor(var1, levels = rev(colnames(cor_pub))),
    var2 = factor(var2, levels = colnames(cor_pub))
  ) %>%
  ggplot(aes(var1, var2, fill = correlation)) +
  geom_tile() + 
  geom_text(aes(label = round(correlation, 2)), size = 5) +
  scale_fill_gradientn(colours = palette_urbn_diverging, na.value = "white", limits = c(-1, 1)) +
  theme_minimal() + 
  theme(text = element_text(size = 20), axis.text.x = element_text(angle = 40, vjust = 1, hjust = 1)) +
  labs(x = NULL, y = NULL) + 
  scale_x_discrete(labels = var) + 
  scale_y_discrete(labels = rev(var)) + 
  labs(fill = "Correlation")

# Correlation matrix for suppressed data - original data
p2 <- bias %>%
  as_tibble(rownames = "var1") %>%
  pivot_longer(cols = -var1, names_to = "var2", values_to = "correlation") %>%
  mutate(
    var1 = factor(var1, levels = rev(colnames(bias))),
    var2 = factor(var2, levels = colnames(bias))
  ) %>%
  ggplot(aes(var1, var2, fill = correlation)) +
  geom_tile() + 
  geom_text(aes(label = round(correlation, 2)), size = 5) +
  scale_fill_gradientn(colours = palette_urbn_diverging, na.value = "white", limits = c(-1, 1)) +
  theme_minimal() + 
  theme(text = element_text(size = 20), axis.text.x = element_text(angle = 40, vjust = 1, hjust = 1)) +
  labs(x = NULL, y = NULL) + 
  scale_x_discrete(labels = var) + 
  scale_y_discrete(labels = rev(var)) + 
  labs(fill = "Correlation Bias")

p1 + p2
```

#### Privacy-Utility Curve

```{r}
#| label: privacy-utility
#| warning: false

# Correlation values New Mexico
nm_bias <- vector()
for(i in 1:k) {
  cor_pub <- nm_supp[[i]][, -(1:2)] %>%
  cor(use = "complete.obs")
  
  cor_pub[upper.tri(cor_pub)] <- NA

  nm_bias[i] <- (cor_pub - cor_nm) %>% abs() %>% mean(na.rm = TRUE) %>% round(digits = 3)
}


# Correlation values Pennsylvania
pa_bias <- vector()
for(i in 1:k) {
  cor_pub <- pa_supp[[i]][, -(1:2)] %>%
  cor(use = "complete.obs")
  
  cor_pub[upper.tri(cor_pub)] <- NA

  pa_bias[i] <- (cor_pub - cor_pa) %>% abs() %>% mean(na.rm = TRUE) %>% round(digits = 3)
}

# Line Plot

nm_cor <- bind_cols(K = 1:k, Bias = nm_bias, State = "New Mexico")

pa_cor <- bind_cols(K = 1:k, Bias = pa_bias, State = "Pennsylvania")

cor_data <- bind_rows(nm_cor, pa_cor) %>%
  mutate(State = fct_relevel(State, 
            "New Mexico", "Pennsylvania"))

ggplot(cor_data, aes(x = K, y = Bias, color = State)) + 
  geom_line() +
  scale_x_continuous(limits = c(1, k),
                     breaks = 1:k) +
  scale_y_continuous(limits = c(0, 0.08)) +
  labs(x = "Suppression Threshold",
       y = "Mean of the Absolute Correlation Bias") + 
  theme(text = element_text(size = 15))

```

## Broadband Access and Single Parent
Our next utility metric is looking at the share of students without access to a computer or broadband internet and share of students with a single parent. We selected these values because are the most correlated with poverty for New Mexico and Pennsylvania, respectively.

### Original Data

#### New Mexico
```{r}
#| label: map-data-nm
#| code-summary: "map-nm"
#| message: false
#| warning: false

# School District Files
schools_nm <- school_districts("New Mexico") %>% invisible()

# Creating the maps
set_urbn_defaults(style = "map")

data_broadband <- nm_prop %>%
    dplyr::select(
      geographic_school_district,
      no_computer_internet
    )

schools_nm <- rename(schools_nm, geographic_school_district = NAME) %>% invisible()

temp <- schools_nm %>%
    dplyr::select(
      geographic_school_district,
      INTPTLAT,
      INTPTLON,
      geometry
    ) %>%
  right_join(data_broadband, by = "geographic_school_district")

ggplot() +
  geom_sf(temp,
          mapping = aes(fill = no_computer_internet),
          color = "#ffffff", size = 0.25) +
  scale_fill_gradientn(labels = scales::percent, limits = c(0, 1)) + 
  labs(fill = "Computer/Broadband")

```

#### Pennsylvania
```{r}
#| label: map-data-pa
#| code-summary: "map-pa"
#| message: false
#| warning: false

# School District Files
schools_pa <- school_districts("Pennsylvania")

# Creating the maps
set_urbn_defaults(style = "map")

pa_proprent <- pa_prop %>%
    dplyr::select(
      geographic_school_district,
      single_parent
    )

schools_pa <- rename(schools_pa, geographic_school_district = NAME)

temp <- schools_pa %>%
    dplyr::select(
      geographic_school_district,
      INTPTLAT,
      INTPTLON,
      geometry
    ) %>%
  right_join(pa_proprent, by = "geographic_school_district")

ggplot() +
  geom_sf(temp,
          mapping = aes(fill = single_parent),
          color = "#ffffff", size = 0.25) +
  scale_fill_gradientn(labels = scales::percent, limits = c(0, 1)) + 
  labs(fill = "Single Parent")
```

### Synthetic Data

#### New Mexico

```{r}
#| label: fig-map-nm-synth
#| fig-cap: "Lack of Computer or Broadband Access in New Mexico (Synthetic Data)"
#| message: false
#| warning: false

source("rcode/broadband_fun.R")

bias <- broadband_fun(nm_synth[[1]], schools_nm, nm_prop)
bias2 <- broadband_fun(nm_synth[[2]], schools_nm, nm_prop)
bias3 <- broadband_fun(nm_synth[[3]], schools_nm, nm_prop)

bias <- bind_rows(bias, bias2, bias3)

n <- nrow(nm_prop)
alpha <- c(0.1, 0.25, 0.5)
alpha <- rep(alpha, each = n)

bias <- bind_cols(bias, alpha)
colnames(bias) <- c("geographic_school_district", "INTPTLAT", "INTPTLON", "no_computer_internet", "Alpha", "geometry")

ggplot() +
  geom_sf(bias,
          mapping = aes(fill = no_computer_internet),
          color = "#ffffff", size = 0.25) +
  scale_fill_gradientn(colors = palette_urbn_diverging, labels = scales::percent, limits = c(-0.25, 0.25)) + 
  labs(fill = "Computer/Broadband Bias") +
    facet_wrap(. ~ Alpha)
  
```

#### Pennsylvania

```{r}
#| label: fig-map-pa-synth
#| fig-cap: "Single Parent in Pennsylvania (Synthetic Data)"
#| message: false
#| warning: false

source("rcode/parent_fun.R")

bias <- parent_fun(pa_synth[[1]], schools_pa, pa_prop)
bias2 <- parent_fun(pa_synth[[2]], schools_pa, pa_prop)
bias3 <- parent_fun(pa_synth[[3]], schools_pa, pa_prop)

bias <- bind_rows(bias, bias2, bias3)

n <- nrow(pa_prop)
alpha <- c(0.1, 0.25, 0.5)
alpha <- rep(alpha, each = n)

bias <- bind_cols(bias, alpha)
colnames(bias) <- c("geographic_school_district", "INTPTLAT", "INTPTLON", "single_parent", "Alpha", "geometry")

ggplot() +
  geom_sf(bias,
          mapping = aes(fill = single_parent),
          color = "#ffffff", size = 0.25) +
  scale_fill_gradientn(colors = palette_urbn_diverging, labels = scales::percent, limits = c(-0.25, 0.25)) + 
  labs(fill = "Single Parent Bias") +
    facet_wrap(. ~ Alpha, dir = "v")

```