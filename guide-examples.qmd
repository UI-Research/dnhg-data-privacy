---
title: "Do No Harm Guide"
subtitle: "Applying Equity Awareness in Data Privacy Methods Examples"
author-title: "Authors"
authors: "Claire McKay Bowen and Joshua Snoke"
affiliation: "Urban Institute and RAND Corp."
date: "today"
format:
  html:
    theme: urbn.scss
    self-contained: true
    code-fold: true
    code-line-numbers: true
    html-math-method: katex
    df-print: default
    toc: true
    toc-depth: 3
    number-sections: true
    number-depth: 3
    highlight-style: pygments
editor_options: 
  chunk_output_type: console
execute: 
  echo: false
---

This Quarto Document walks through "Do No Harm Guide: Applying Equity Awareness in Data Privacy Methods".

**Disclaimer:** These analyses are inspired by real data and public policy analytics, but are *not* representative of them.

```{r}
#| label: load-pkgs
#| code-summary: "Packages"
#| message: false
#| warning: false
#| echo: false
library(tidyverse)  # for data wrangling and visualization
library(knitr)      # for tables
library(readxl)     # for importing excel (data)
library(smoothmest) # for laplace distribution
library(urbnthemes) # for ggplot2 theme
library(tigris)     # for census shape files
library(doParallel) # for parallel computing

cl <- makeCluster(detectCores() - 4)
clusterCall(cl, function() lapply(c("doParallel", "tidyverse"), require, character.only = TRUE))
registerDoParallel(cl)
set_urbn_defaults(style = "print")

```

## Data

The data are from a Urban research report called, "[Mapping Student Needs during COVID-19](https://www.urban.org/research/publication/mapping-student-needs-during-covid-19)" and can be accessed at [Urban Data Catalog](https://datacatalog.urban.org/dataset/household-conditions-geographic-school-district). The purpose of the research is to "...highlight different types of challenges to remote learning and point to district and educator strategies that mitigate harm to students as districts navigate long-term school closures."

In addition to poverty, the researchers focused on six other factors: linguistic isolation, child disability status, parents in vulnerable economic sectors, single parent, crowded conditions, and lack of computer or broadband access. The researchers used 2014 through 2018 data from the American Community Survey (ACS) to conduct their analyses.

### Confidential Data (Gold Standard Data)
The ACS is a "...nationally representative one percent survey conducted annually by the US Census Bureau, containing data on households, demographics, economic circumstances, education, housing conditions, and more." The researchers used two versions of the ACS. We use one of the datasets and treat the data as the confidential data. We define the confidential data as the cleaned version (meaning edited for inaccuracies or inconsistencies) of the data; often referred to as the gold standard or actual data for analysis.

The dataset we used is the 2014 through 2018 five year estimate from the National Historical Geographic Information System (NHGIS). The NHGIS data has the seven variables of interest as proportions with margins of error. For simplicity, we assume the proportions are the true values and create another version of the data into tabular values based on those proportions. We then readjust the values to ensure there is whole counts. For this guide, we focus on two states, New Mexico and Pennsylvania, because these states vary greatly in population density and the proportion of the seven variables.

**Note that we apply these data to multiple analyses that are inspired from the original Urban report, but are not the original analyses.**

```{r}
#| label: tabular-data
#| code-summary: "tabular"
#| message: false
#| warning: false

# New Mexico Data
data_nm <- read_excel("data/data/nhgis_district_data_var.xlsx") %>%
  filter(state == "New Mexico") %>%
  dplyr::select(
    geographic_school_district,
    children_5_17,
    poverty,
    linguistically_isolated,
    children_disability,
    vulnerable_job,
    single_parent,
    crowded_conditions, 
    no_computer_internet
  )

# Pennsylvania Data
data_pa <- read_excel("data/data/nhgis_district_data_var.xlsx") %>%
  filter(state == "Pennsylvania") %>%
  dplyr::select(
    geographic_school_district,
    children_5_17,
    poverty,
    linguistically_isolated,
    children_disability,
    vulnerable_job,
    single_parent,
    crowded_conditions, 
    no_computer_internet
  ) %>%
  head(-1) # Empty district

# New Mexico Count Data
nm_counts <- data_nm %>%
  mutate(
    poverty = round(poverty * children_5_17),
    linguistically_isolated = round(linguistically_isolated * children_5_17),
    children_disability = round(children_disability * children_5_17),
    vulnerable_job = round(vulnerable_job * children_5_17),
    single_parent = round(single_parent * children_5_17),
    crowded_conditions = round(crowded_conditions * children_5_17), 
    no_computer_internet = round(no_computer_internet * children_5_17)
  )

# Pennsylvania Count Data
pa_counts <- data_pa %>%
  mutate(
    poverty = round(poverty * children_5_17),
    linguistically_isolated = round(linguistically_isolated * children_5_17),
    children_disability = round(children_disability * children_5_17),
    vulnerable_job = round(vulnerable_job * children_5_17),
    single_parent = round(single_parent * children_5_17),
    crowded_conditions = round(crowded_conditions * children_5_17), 
    no_computer_internet = round(no_computer_internet * children_5_17)
  )
```

### Statistical Disclosure Control Methods
In this section, we apply the current three most popular data privacy and confidentiality methods:

  1. **Suppression** - removing values via k-anonymity.
  2. **Synthetic Data** - replicating the confidential data based on a statistically representative model to generate pseudo or fake data records
  3. **Noise infusion** - differential privacy or differentially private method

#### Suppression
Suppression, or not reporting certain values from the data, is one of the earliest and easiest statistical disclosure control methods. A version of suppression is $k$-anonymity, which requires that any released data set contain at least $k$ observations for each combination of possibly identifying variables (e.g., age, sex, or race). We can alter or suppress the data to achieve a certain level of $k$-anonymity.

In our case, we will suppress the data up to $k$ = 10 for each of the seven factors. Typically, $k$ = 3, but we picked the upper bound to be a higher value because the data we are treating as the confidential data has been altered for privacy. However, in one of our examples, we show how changing the value of $k$ affects the data privacy-utility trade-off.

```{r}
#| label: suppression
#| code-summary: "suppression"
#| message: false
#| warning: false

source("rcode/k-suppression.R")

k <- 10

# Suppression on New Mexico Data
nm_supp <- lapply(1:k, function(x) k_suppresion(nm_counts, x))

# Suppression on Pennsylvania Data
pa_supp <- lapply(1:k, function(x) k_suppresion(pa_counts, x))

```

#### Synthetic Data
Synthetic data consists of pseudo or “fake” records that are statistically representative of the confidential data. Records are considered synthesized when they are replaced with draws from a model fitted to the confidential data.

Fully synthetic data synthesizes all values in the dataset with imputed amounts, which means the data no longer directly map onto the confidential records. Since fully synthetic data does not contain any actual observations, the synthetic data protects against both attribute and identity disclosure risks.

There are generally two ways of generating synthetic data: parametric and nonparametric.

 - Parametric models assume a finite number of parameters that capture the complexity of the data.
 - They are generally less flexible, but more interpretable than nonparametric models.
 - Examples: regression to assign an age variable, sampling from a probability distribution, Bayesian models, copula based models.

Nonparametric data synthesis is the process of data generation that is not based on assumptions about an underlying distribution or model.

 - Often, nonparametric methods use frequency proportions or marginal probabilities as weights for some type of sampling scheme.
 - They are generally more flexible, but less interpretable than parametric models.
 - Examples: assigning gender based on underlying proportions, CART (Classification and Regression Trees) models, RNN models, etc.

We will generate fully synthetic data via a nonparametric model for each factor and total children count. We have a postprocessing step (i.e., ensuring the results of the statistic or information are consistent with realistic constraints) to ensure that the total count is greater than or equal to max count of any factor. We also assume zeros (i.e., if there is a zero in the data, we do not alter it).

A drawback to this approach is that we are synthesizing each variable individually. The univariate properties will likely be preserved, but not the multivariate.

```{r}
#| label: synthetic
#| code-summary: "synthetic"
#| message: false
#| warning: false

# Synthetic data based on the proportions for each factor
source("rcode/synth-count.R")
source("rcode/post-processing.R")
source("rcode/synth.R")

clusterCall(cl, function() {source("rcode/synth-count.R")})
clusterCall(cl, function() {source("rcode/post-processing.R")})
clusterCall(cl, function() {source("rcode/synth.R")})

set.seed(20230128)
n <- 10

# Synthetic New Mexico Data
temp <- foreach(i=1:n) %dopar% synth(nm_counts)
nm_synth <- temp[[1]]

for(i in 2:n) {
  nm_synth[, 2:ncol(nm_counts)] <- nm_synth[, 2:ncol(nm_counts)] + temp[[i]][, 2:ncol(nm_counts)]
}
nm_synth[, 2:ncol(nm_counts)] <- nm_synth[, 2:ncol(nm_counts)] / n

# Synthetic Pennsylvania Data
temp <- foreach(i=1:n) %dopar% synth(pa_counts)
pa_synth <- temp[[1]]

for(i in 2:n) {
  pa_synth[, 2:ncol(pa_counts)] <- pa_synth[, 2:ncol(pa_counts)] + temp[[i]][, 2:ncol(pa_counts)]
}
pa_synth[, 2:ncol(pa_counts)] <- pa_synth[, 2:ncol(pa_counts)] / n

stopCluster(cl)

```

#### DP synthetic - Laplace sanitizer
At a high level, differential privacy (DP) is a strict mathematical definition that a method must satisfy (or meet the mathematical conditions) to be considered differentially private, not a statement or description of the data itself. 

A sanitizer that satisfies DP is the Laplace mechanism, which adds noise by drawing values from a Laplace distribution. The Laplace distribution is centered at zero and the distribution variability (i.e., wide or narrow the distribution is) is the ratio of the privacy loss budget, $\epsilon$, over the sensitivity of the target statistics. Having the distribution centered at zero means there is a higher probability of adding very little or no noise to the confidential data statistics. For the noise variability, if $\epsilon$ is large or the sensitivity of the statistic is low, then there is a higher probability of adding very little noise to confidential data statistic. If $\epsilon$ is small or the sensitivity of the statistic probability of adding a lot of noise to the released statistic.

For our example, we will add Laplace noise to each count and normalize to the totals. In other words, we assume the total counts are invariant (i.e., no change to the statistics). We will test for $\epsilon$ = 1.

Similar to the synthetic data method, we have a postprocessing step to ensure that the total count is greater than or equal to max count of any factor. We also assume zeros.

```{r}
#| label: dp-laplace
#| code-summary: "laplace-mechanism"
#| message: false
#| warning: false

# Differentially private synthetic data based on the proportions for each factor.
source("rcode/dp-count.R")
source("rcode/helper-functions.R")
source("rcode/post-processing.R")
source("rcode/lap-san.R")

set.seed(20230128)
n <- 10

eps <- 0.01

# Differentially private synthetic New Mexico Data
temp <- lapply(1:n, function(x) lap_san(nm_counts, eps))
nm_dp <- temp[[1]]

for(i in 2:n) {
  nm_dp[, 2:ncol(nm_counts)] <- nm_dp[, 2:ncol(nm_counts)] + temp[[i]][, 2:ncol(nm_counts)]
}
nm_dp[, 2:ncol(nm_counts)] <- nm_dp[, 2:ncol(nm_counts)] / n

# Differentially private synthetic Pennsylvania Data
temp <- lapply(1:n, function(x) lap_san(pa_counts, eps))
pa_dp <- temp[[1]]

for(i in 2:n) {
  pa_dp[, 2:ncol(pa_counts)] <- pa_dp[, 2:ncol(pa_counts)] + temp[[i]][, 2:ncol(pa_counts)]
}
pa_dp[, 2:ncol(pa_counts)] <- pa_dp[, 2:ncol(pa_counts)] / n

```

## Line Plots
We first look at how the values for each factor are distributed. Specifically, we examine the distribution of the share of students in poverty since the Urban report looked at poverty levels.

### New Mexico Poverty

```{r}
#| label: fig-nm-line
#| fig-cap: "Comparing Poverty Rates across School Districts in New Mexico"
#| warning: false
#| message: false

# Creating the line plot
original <- data_nm %>% 
  dplyr::select(poverty)
dp <- nm_dp %>% 
  dplyr::select(poverty)

line_data <- bind_cols(original, dp)
names(line_data) <- c("Original", "DP")

ggplot(line_data, aes(Original, DP)) +
  geom_abline() +
  geom_point(alpha = 0.2) +
  coord_equal() +
  scale_x_continuous(limits = c(0, 1)) +
  scale_y_continuous(limits = c(0, 1)) +
  labs(
    title = "Comparing Poverty Rates across School Districts in New Mexico",
    x = "Original Data Values",
    y = "Differentially Private Values"
  )
```

### Pennsylvania Poverty

```{r}
#| label: fig-pa-line
#| fig-cap: "Comparing Poverty Rates across School Districts in Pennsylvania"
#| warning: false
#| message: false

# Creating the line plot
original <- data_pa %>% 
  dplyr::select(poverty)
dp <- pa_dp %>% 
  dplyr::select(poverty)

line_data <- bind_cols(original, dp)
names(line_data) <- c("Original", "DP")

ggplot(line_data, aes(Original, DP)) +
  geom_abline() +
  geom_point(alpha = 0.2) +
  coord_equal() +
  scale_x_continuous(limits = c(0, 1)) +
  scale_y_continuous(limits = c(0, 1)) +
  labs(
    title = "Comparing Poverty Rates across School Districts in Pennsylvania",
    x = "Original Data Values",
    y = "Differentially Private Values"
  )
```

## Correlation Matrix
Our next utility metric is creating a correlation matrix. The Urban report focused on what factors were highly correlated with poverty. We repeat this analysis for our New Mexico and Pennsylvania.

### Original Data

#### New Mexico
For New Mexico, we see that "lack of computer or broadband access" is most correlated (0.53) with "single parent" is next (0.39). The other factors would be considered weak, where the values are under 0.15 (the next largest at 0.12).

```{r}
#| label: fig-nm-cor
#| fig-cap: "Correlations between Measures of Vulnerability across School Districts in New Mexico"
#| warning: false

# Correlation values
cor_nm <- data_nm[, -(1:2)] %>%
  cor()
cor_nm[upper.tri(cor_nm)] <- NA

# Correlation matrix
cor_nm %>%
  as_tibble(rownames = "var1") %>%
  pivot_longer(cols = -var1, names_to = "var2", values_to = "correlation") %>%
  mutate(
    var1 = factor(var1, levels = rev(colnames(cor_nm))),
    var2 = factor(var2, levels = colnames(cor_nm))
  ) %>%
  ggplot(aes(var1, var2, fill = correlation)) +
  geom_tile() + 
  geom_text(aes(label = round(correlation, 2))) +
  scale_fill_gradientn(colours = palette_urbn_diverging, na.value = "white", limits = c(-1, 1)) +
  theme_minimal() + 
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
  labs(x = NULL, y = NULL)
```

#### Pennsylvania
For Pennsylvania, we see that "single parent" is most correlated (0.71) with "child disability status" and "lack of computer or broadband access" next (0.53 and 0.48, respectively).

```{r}
#| label: fig-pa-cor
#| fig-cap: "Correlations between Measures of Vulnerability across School Districts in Pennsylvania"
#| warning: false

# Correlation values
cor_pa <- data_pa[, -(1:2)] %>%
  cor()
cor_pa[upper.tri(cor_pa)] <- NA

# Correlation matrix
cor_pa %>%
  as_tibble(rownames = "var1") %>%
  pivot_longer(cols = -var1, names_to = "var2", values_to = "correlation") %>%
  mutate(
    var1 = factor(var1, levels = rev(colnames(cor_pa))),
    var2 = factor(var2, levels = colnames(cor_pa))
  ) %>%
  ggplot(aes(var1, var2, fill = correlation)) +
  geom_tile() + 
  geom_text(aes(label = round(correlation, 2))) +
  scale_fill_gradientn(colours = palette_urbn_diverging, na.value = "white", limits = c(-1, 1)) +
  theme_minimal() + 
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
  labs(x = NULL, y = NULL)
```

### Suppressed Data

#### New Mexico
For the New Mexico suppressed data, we see that "lack of computer or broadband access" is still the most correlated (0.61) with "single parent" is next (0.44). The overall L1 difference on the correlation matrix is 1.33.

```{r}
#| label: fig-cor-suppressed-nm
#| fig-cap: "Correlations between Measures of Vulnerability across School Districts in New Mexico (Suppressed, k = 10)"
#| fig-subcap:
#|   - "Correlations (Suppressed, k = 10)"
#|   - "Correlations Bias" 
#| warning: false

# Correlation values New Mexico

# Plotting for k = 10
cor_pub <- nm_supp[[k]][, -(1:2)] %>%
  cor()
cor_pub[upper.tri(cor_pub)] <- NA

bias <- cor_pub - cor_nm

bias %>% abs() %>% sum(na.rm = TRUE) %>% round(digits = 2)

# Correlation matrix for suppressed data
cor_pub %>%
  as_tibble(rownames = "var1") %>%
  pivot_longer(cols = -var1, names_to = "var2", values_to = "correlation") %>%
  mutate(
    var1 = factor(var1, levels = rev(colnames(cor_pub))),
    var2 = factor(var2, levels = colnames(cor_pub))
  ) %>%
  ggplot(aes(var1, var2, fill = correlation)) +
  geom_tile() + 
  geom_text(aes(label = round(correlation, 2))) +
  scale_fill_gradientn(colours = palette_urbn_diverging, na.value = "white", limits = c(-1, 1)) +
  theme_minimal() + 
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
  labs(x = NULL, y = NULL)

# Correlation matrix for suppressed data - original data
bias %>%
  as_tibble(rownames = "var1") %>%
  pivot_longer(cols = -var1, names_to = "var2", values_to = "correlation") %>%
  mutate(
    var1 = factor(var1, levels = rev(colnames(bias))),
    var2 = factor(var2, levels = colnames(bias))
  ) %>%
  ggplot(aes(var1, var2, fill = correlation)) +
  geom_tile() +
  geom_text(aes(label = round(correlation, 2))) +
  scale_fill_gradientn(colours = palette_urbn_diverging, na.value = "white", limits = c(-1, 1)) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
  labs(x = NULL, y = NULL)
```

#### Pennsylvania
For the Pennsylvania suppressed data, we see that "single parent" is still the most correlated (0.71) with "children with disability" and "lack of computer or broadband access" next (0.52 and 0.48, respectively). These values are almost exactly the same as the original data, which is expected because Pennsylvania has a fairly dense population. The overall L1 difference on the correlation matrix is 0.21.

```{r}
#| label: fig-cor-suppressed-pa
#| fig-cap: "Correlations between Measures of Vulnerability across School Districts in Pennsylvania (Suppressed, k = 10)"
#| fig-subcap:
#|   - "Correlations (Suppressed, k = 10)"
#|   - "Correlations Bias" 
#| warning: false

# Correlation values Pennsylvania
cor_pub <- pa_supp[[k]][, -(1:2)] %>%
  cor()
cor_pub[upper.tri(cor_pub)] <- NA

bias <- cor_pub - cor_pa

bias %>% abs() %>% sum(na.rm = TRUE) %>% round(digits = 2)

# Correlation matrix for suppressed data
cor_pub %>%
  as_tibble(rownames = "var1") %>%
  pivot_longer(cols = -var1, names_to = "var2", values_to = "correlation") %>%
  mutate(
    var1 = factor(var1, levels = rev(colnames(cor_pub))),
    var2 = factor(var2, levels = colnames(cor_pub))
  ) %>%
  ggplot(aes(var1, var2, fill = correlation)) +
  geom_tile() + 
  geom_text(aes(label = round(correlation, 2))) +
  scale_fill_gradientn(colours = palette_urbn_diverging, na.value = "white", limits = c(-1, 1)) +
  theme_minimal() + 
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
  labs(x = NULL, y = NULL)

# Correlation matrix for suppressed data - original data
bias %>%
  as_tibble(rownames = "var1") %>%
  pivot_longer(cols = -var1, names_to = "var2", values_to = "correlation") %>%
  mutate(
    var1 = factor(var1, levels = rev(colnames(bias))),
    var2 = factor(var2, levels = colnames(bias))
  ) %>%
  ggplot(aes(var1, var2, fill = correlation)) +
  geom_tile() +
  geom_text(aes(label = round(correlation, 2))) +
  scale_fill_gradientn(colours = palette_urbn_diverging, na.value = "white", limits = c(-1, 1)) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
  labs(x = NULL, y = NULL)
```

#### Privacy-Utility Curve

```{r}
#| label: privacy-utility
#| warning: false

# Correlation values New Mexico
nm_bias <- vector()
for(i in 1:k) {
  cor_pub <- nm_supp[[i]][, -(1:2)] %>%
  cor()
  
  cor_pub[upper.tri(cor_pub)] <- NA

  nm_bias[i] <- (cor_pub - cor_nm) %>% abs() %>% sum(na.rm = TRUE) %>% round(digits = 2)
}

# Correlation values Pennsylvania
pa_bias <- vector()
for(i in 1:k) {
  cor_pub <- pa_supp[[i]][, -(1:2)] %>%
  cor()
  
  cor_pub[upper.tri(cor_pub)] <- NA

  pa_bias[i] <- (cor_pub - cor_pa) %>% abs() %>% sum(na.rm = TRUE) %>% round(digits = 2)
}

# Line Plot

nm_cor <- bind_cols(K = 1:k, Bias = nm_bias, State = "New Mexico")

pa_cor <- bind_cols(K = 1:k, Bias = pa_bias, State = "Pennsylvania")

cor_data <- bind_rows(nm_cor, pa_cor) %>%
  mutate(State = fct_relevel(State, 
            "New Mexico", "Pennsylvania"))

ggplot(cor_data, aes(x = K, y = Bias, color = State)) + 
  geom_line() +
  scale_x_continuous(limits = c(1, k),
                     breaks = 1:k) +
  scale_y_continuous(limits = c(0, 1.5),
                     breaks = 0:15 * 0.1) +
  labs(x = "K",
       y = "Correlation Bias")

```

## Broadband Access and Single Parent
Our next utility metric is looking at the share of students without access to a computer or broadband internet and share of students with a single parent. We selected these values because are the most correlated with poverty for New Mexico and Pennsylvania, respectively.

### Original Data

#### New Mexico
```{r}
#| label: map-data-nm
#| code-summary: "map-nm"
#| message: false
#| warning: false

# School District Files
schools_nm <- school_districts("New Mexico")

# Creating the maps
set_urbn_defaults(style = "map")

data_broadband <- data_nm %>%
    dplyr::select(
      geographic_school_district,
      no_computer_internet
    )

schools_nm <- rename(schools_nm, geographic_school_district = NAME)

temp <- schools_nm %>%
    dplyr::select(
      geographic_school_district,
      INTPTLAT,
      INTPTLON,
      geometry
    ) %>%
  right_join(data_broadband, by = "geographic_school_district")

ggplot() +
  geom_sf(temp,
          mapping = aes(fill = no_computer_internet),
          color = "#ffffff", size = 0.25) +
  scale_fill_gradientn(labels = scales::percent, limits = c(0, 1))

```

#### Pennsylvania
```{r}
#| label: map-data-pa
#| code-summary: "map-pa"
#| message: false
#| warning: false

# School District Files
schools_pa <- school_districts("Pennsylvania")

# Creating the maps
set_urbn_defaults(style = "map")

data_parent <- data_pa %>%
    dplyr::select(
      geographic_school_district,
      single_parent
    )

schools_pa <- rename(schools_pa, geographic_school_district = NAME)

temp <- schools_pa %>%
    dplyr::select(
      geographic_school_district,
      INTPTLAT,
      INTPTLON,
      geometry
    ) %>%
  right_join(data_parent, by = "geographic_school_district")

ggplot() +
  geom_sf(temp,
          mapping = aes(fill = single_parent),
          color = "#ffffff", size = 0.25) +
  scale_fill_gradientn(labels = scales::percent, limits = c(0, 1))
```

### Synthetic Data

#### New Mexico
```{r}
#| label: fig-map-nm-synth
#| fig-cap: "Lack of Computer or Broadband Access in New Mexico"
#| fig-subcap:
#|   - "Broadband Count (Synthetic)"
#|   - "Broadband Count Bias" 
#| message: false
#| warning: false

data_broadband <- nm_synth %>%
    dplyr::select(
      geographic_school_district,
      no_computer_internet
    )

temp <- schools_nm %>%
    dplyr::select(
      geographic_school_district,
      INTPTLAT,
      INTPTLON,
      geometry
    ) %>%
  right_join(data_broadband, by = "geographic_school_district")

bias <- data_broadband$no_computer_internet - data_nm$no_computer_internet

bias %>% abs() %>% sum()

bias <- bind_cols(temp$geographic_school_district, bias)
colnames(bias) <- c("geographic_school_district", "no_computer_internet")

bias <- schools_nm %>%
    dplyr::select(
      geographic_school_district,
      INTPTLAT,
      INTPTLON,
      geometry
    ) %>%
  right_join(bias, by = "geographic_school_district")

ggplot() +
  geom_sf(temp,
          mapping = aes(fill = no_computer_internet),
          color = "#ffffff", size = 0.25) +
  scale_fill_gradientn(labels = scales::percent, limits = c(0, 1))

ggplot() +
  geom_sf(bias,
          mapping = aes(fill = no_computer_internet),
          color = "#ffffff", size = 0.25) +
  scale_fill_gradientn(colors = palette_urbn_diverging, labels = scales::percent, limits = c(-0.5, 0.5))
```

#### Pennsylvania
In Pennsylvania, the overall absolute difference is 0.587 for the share of students without access to a computer or broadband internet.
```{r}
#| label: fig-map-pa-dp
#| fig-cap: "Single Parent in Pennsylvania"
#| fig-subcap:
#|   - "Broadband Count (DP)"
#|   - "Broadband Count Bias" 
#| message: false
#| warning: false

data_single <- pa_dp %>%
    dplyr::select(
      geographic_school_district,
      single_parent
    )

temp <- schools_pa %>%
    dplyr::select(
      geographic_school_district,
      INTPTLAT,
      INTPTLON,
      geometry
    ) %>%
  right_join(data_single, by = "geographic_school_district")

bias <- data_single$single_parent - data_pa$single_parent

bias %>% abs() %>% sum()

bias <- bind_cols(temp$geographic_school_district, bias)
colnames(bias) <- c("geographic_school_district", "single_parent")

bias <- schools_pa %>%
    dplyr::select(
      geographic_school_district,
      INTPTLAT,
      INTPTLON,
      geometry
    ) %>%
  right_join(bias, by = "geographic_school_district")

ggplot() +
  geom_sf(temp,
          mapping = aes(fill = single_parent),
          color = "#ffffff", size = 0.25) +
  scale_fill_gradientn(labels = scales::percent, limits = c(0, 1))

ggplot() +
  geom_sf(bias,
          mapping = aes(fill = single_parent),
          color = "#ffffff", size = 0.25) +
  scale_fill_gradientn(colors = palette_urbn_diverging, labels = scales::percent, limits = c(-0.5, 0.5))
```